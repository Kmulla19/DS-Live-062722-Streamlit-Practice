{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Animal Adoptions\n",
    "\n",
    "### Project Goal:\n",
    "\n",
    "Using a few simple inputs (related to animal type, age of animal, color, etc), build an online tool that will predict whether an animal will be adopted or not from the Austin Animal Center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 25)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, roc_auc_score, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration and Merging\n",
    "\n",
    "Data Sources:\n",
    "- [Austin Animal Center Intakes Data](https://data.austintexas.gov/Health-and-Community-Services/Austin-Animal-Center-Intakes/wter-evkm)\n",
    "- [Austin Animal Center Outcomes Data](https://data.austintexas.gov/Health-and-Community-Services/Austin-Animal-Center-Outcomes/9t4d-g238)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in intakes data, downloaded 9/19/22\n",
    "df_in = pd.read_csv('data/Austin_Animal_Center_Intakes-091922.csv',\n",
    "                    parse_dates=['DateTime'])\n",
    "df_in.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in outcomes data, downloaded 9/19/22\n",
    "df_out = pd.read_csv('data/Austin_Animal_Center_Outcomes-091922.csv', \n",
    "                     parse_dates=['DateTime'])\n",
    "df_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# There are some duplicate rows with the exact same Animal ID and DateTime\n",
    "# Assuming input issues (ie accidentally entering the animal into the system twice)\n",
    "print(f\"Intake dupes: {df_in.duplicated(subset=['Animal ID', 'DateTime']).sum()}\")\n",
    "print(f\"Outcome dupes: {df_out.duplicated(['Animal ID', 'DateTime']).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping these dupes, keeping the last entry\n",
    "df_in = df_in.drop_duplicates(subset=['Animal ID', 'DateTime'], keep='last')\n",
    "df_out = df_out.drop_duplicates(subset=['Animal ID', 'DateTime'], keep='last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning: Merging Repeat Intakes/Outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First - order both dfs by DateTime\n",
    "df_in = df_in.sort_values(by='DateTime')\n",
    "df_out = df_out.sort_values(by='DateTime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create iterative counts for animals that have multiple intakes/outcomes\n",
    "df_in['Intake Num'] = df_in.groupby('Animal ID', sort=False).cumcount()+1\n",
    "df_out['Outcome Num'] = df_out.groupby('Animal ID', sort=False).cumcount()+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Checking a known repeat offender - multiple intakes ...\n",
    "df_in.loc[df_in['Animal ID'] == 'A721033'].tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ... and multiple outcomes\n",
    "df_out.loc[df_out['Animal ID'] == 'A721033'].tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try merging using these new iterative count columns\n",
    "df = df_in.merge(df_out, \n",
    "                 left_on=['Animal ID', 'Intake Num'], \n",
    "                 right_on=['Animal ID', 'Outcome Num'],\n",
    "                 how='inner',\n",
    "                 suffixes=(\"_in\", \"_out\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring rows where the simple iterative count didn't work \n",
    "# Aka the outcome date was before the intake date\n",
    "dirty = df.loc[df['DateTime_in'] > df['DateTime_out']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ew - this many rows didn't quite work with a simple iterative count\n",
    "dirty.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsetting down, for simplicity\n",
    "dirty = dirty[['Animal ID', 'DateTime_in', 'Intake Num']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going to add 1 to intake num, then try to merge again\n",
    "dirty['Intake Num'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's get a clean version of df_in ready\n",
    "df_in_clean = df_in.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding columns just on the dirty rows using a left merge\n",
    "# The Intake Num with \"first\" was the 1st attempt, \"second\" is 2nd attempt\n",
    "df_in_clean = df_in_clean.merge(dirty,\n",
    "                                left_on=['Animal ID', 'DateTime'], \n",
    "                                right_on=['Animal ID', 'DateTime_in'], \n",
    "                                how='left',\n",
    "                                suffixes=('_first', '_second'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't need an extra datetime column anymore\n",
    "df_in_clean = df_in_clean.drop(columns=['DateTime_in'])\n",
    "# If we check, we can see that non-null count in Intake Num_second\n",
    "# matches the number of dirty rows we found above\n",
    "df_in_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new Intake Num column\n",
    "# Using np.where to take the second num when not null, else the first\n",
    "df_in_clean['Intake Num'] = np.where(~df_in_clean['Intake Num_second'].isna(),\n",
    "                                     df_in_clean['Intake Num_second'],\n",
    "                                     df_in_clean['Intake Num_first'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now trying the big merge again, using the new Intake Num\n",
    "df = df_in_clean.merge(df_out, \n",
    "                       left_on=['Animal ID', 'Intake Num'], \n",
    "                       right_on=['Animal ID', 'Outcome Num'],\n",
    "                       how='inner',\n",
    "                       suffixes=(\"_in\", \"_out\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-checking for rows with outcome date before intake date\n",
    "df.loc[df['DateTime_in'] > df['DateTime_out']] # yassssss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmenting down to mostly intake columns that I'll explore for modeling\n",
    "data = df[['Animal ID', 'DateTime_in', 'Intake Type', 'Intake Condition', \n",
    "           'Animal Type_in', 'Sex upon Intake', 'Age upon Intake', 'Breed_in', \n",
    "           'Color_in', 'Intake Num', 'DateTime_out', 'Date of Birth', 'Outcome Type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Outcome Type - aka Target Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Outcome Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluding animals that were returned to owner in some way\n",
    "data = data.loc[(data['Outcome Type'] != 'Return to Owner') & (data['Outcome Type'] != 'Rto-Adopt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Outcome Type'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Creating our target column, which is a binary (was either adopted or not)\n",
    "data['Adopted'] = np.where(data['Outcome Type'] == 'Adoption', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Adopted'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Object-Type Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.describe(include='O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Getting a list of object-type columns\n",
    "obj_cols = [c for c in data.columns if data[c].dtype == 'O']\n",
    "\n",
    "# Looping over object-type columns (except Animal ID)\n",
    "# Checking out the top 10 of the value counts\n",
    "for col in obj_cols[1:]:\n",
    "    print(col)\n",
    "    print(f\"Uniques: {len(data[col].value_counts())}\")\n",
    "    print(data[col].value_counts().head(10))\n",
    "    print(\"*\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Age in Days from date of birth\n",
    "data['Age in Days'] = (pd.Timestamp.today().date() - pd.to_datetime(data['Date of Birth']).dt.date).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an indicator for black animals (notoriously under-adopted)\n",
    "data['Color_black'] = data['Color_in'].str.lower().str.contains('black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a 'fixed' col for animals that come in fixed\n",
    "data['Fixed'] = np.where(\n",
    "    (data['Sex upon Intake'] == 'Neutered Male') | (data['Sex upon Intake'] == 'Spayed Female'), True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mainly looking at dogs and cats - rest will be 'Other'\n",
    "data['Animal Type_in'] = data['Animal Type_in'].replace({'Bird': 'Other', 'Livestock': 'Other'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Type_Cat and Type_Dog columns\n",
    "data['Type_Cat'] = data['Animal Type_in'] == 'Cat'\n",
    "data['Type_Dog'] = data['Animal Type_in'] == 'Dog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding not-normal intake conditions\n",
    "data['Intake Condition_Not Normal'] = data['Intake Condition'] != 'Normal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a 'female' indicator\n",
    "data['Female'] = data['Sex upon Intake'].str.contains(\"Female\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplifying the age in days column to extract animals < 1yo\n",
    "data['Young'] = data['Age in Days'] < 365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our used calls - all indicator boolean columns!\n",
    "used_cols = ['Color_black', 'Fixed', 'Type_Cat', 'Type_Dog', \n",
    "             'Intake Condition_Not Normal', 'Female', 'Young']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[used_cols]\n",
    "y = data['Adopted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=84)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-Less Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "logreg.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = logreg.predict(X_test)\n",
    "test_probas = logreg.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(f\"Test Acc: {logreg.score(X_test, y_test)}\")\n",
    "print(f\"Test F1: {f1_score(y_test, test_preds)}\")\n",
    "print(f\"Test ROCAUC: {roc_auc_score(y_test, test_probas)}\")\n",
    "\n",
    "plot_confusion_matrix(logreg, X_test, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "tree.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = tree.predict(X_test)\n",
    "test_probas = tree.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(f\"Test Acc: {tree.score(X_test, y_test)}\")\n",
    "print(f\"Test F1: {f1_score(y_test, test_preds)}\")\n",
    "print(f\"Test ROCAUC: {roc_auc_score(y_test, test_probas)}\")\n",
    "\n",
    "plot_confusion_matrix(tree, X_test, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(max_depth=5)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "rf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = rf.predict(X_test)\n",
    "test_probas = rf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(f\"Test Acc: {rf.score(X_test, y_test)}\")\n",
    "print(f\"Test F1: {f1_score(y_test, test_preds)}\")\n",
    "print(f\"Test ROCAUC: {roc_auc_score(y_test, test_probas)}\")\n",
    "\n",
    "plot_confusion_matrix(rf, X_test, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd prefer to false negatives over false positives: would rather an animal be predicted to not be adopted, but actually is, rather than one that's predicted to be adopted but isn't.\n",
    "\n",
    "Because of that, and a marginally higher ROC-AUC score, we'll choose our (mostly untuned and could definitely be improved) Random Forest model to pickle and deploy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New library!\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save our model as a .sav file, use this code\n",
    "# Note that rf is the variable of our model to save\n",
    "pickle.dump(rf, open(\"rf_model.sav\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that out, too!\n",
    "# Loading up the model from the .sav file\n",
    "loaded_model = pickle.load(open(\"rf_model.sav\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We can see it's the same as above\n",
    "loaded_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing On a New Input\n",
    "\n",
    "(aka what we'll need in our streamlit app!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll need to copy this over to streamlit\n",
    "print(used_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want a new intake to look like this\n",
    "X_test[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needs to match these dtypes too\n",
    "X_test.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example predictions on just one \n",
    "loaded_model.predict(X_test[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.predict_proba(X_test[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example row of new inputs\n",
    "example_row = [True, False, False, True, False, False, True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning that into a dataframe\n",
    "new_test_example = pd.DataFrame(dict(zip(used_cols, example_row)), index=[0])\n",
    "new_test_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proving we can use our model to predict on that!\n",
    "loaded_model.predict(new_test_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
